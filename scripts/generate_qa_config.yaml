global:
  base_dir: qa_generation_input
  output_dir: qa_generation_output
  output_base_path: /var/kolo_data
  ollama_url: http://localhost:11434/api/generate

providers:
  question:
    provider: openai
    model: gpt-4o-mini
  answer:
    provider: ollama
    model: llama3.1
    # Note: The URL defined here (if any) will be ignored in favor of the global ollama_url.

prompts:
  question_header: 'Generate a list of questions that cover the full contents of the file.'
  question_footer: |
    --- Output ---
    1.
    2.
    3.
  question_file_header: 'Each question must include "{file_name}"'
  question_group_content: '{files_content}'
  answer_header: 'Based on the file content provided, answer the following question in detail.'

file_groups:
  BuildImage:
    iterations: 15
    files:
      - build_image.ps1
      - dockerfile
      - supervisord.conf
  UninstallModel:
    iterations: 5
    files:
      - uninstall_model.ps1
  DeleteModel:
    iterations: 5
    files:
      - delete_model.ps1
  RunContainer:
    iterations: 5
    files:
      - create_and_run_container.ps1
      - run_container.ps1
  TrainUnsloth:
    iterations: 15
    files:
      - train_model_unsloth.ps1
      - train.py
  TrainTorchTune:
    iterations: 15
    files:
      - train_model_torchtune.ps1
      - merge_lora.py
      - convert_jsonl_to_json.py
  TrainingPSCommands:
    iterations: 15
    files:
      - train_model_torchtune.ps1
      - train_model_unsloth.ps1
  InstallModel:
    iterations: 5
    files:
      - install_model.ps1
  ListModels:
    iterations: 5
    files:
      - list_models.ps1
  CopyScripts:
    iterations: 5
    files:
      - copy_scripts.ps1
  CopyConfigs:
    iterations: 5
    files:
      - copy_configs.ps1
  ConnectSSH:
    iterations: 5
    files:
      - connect.ps1
  FineTuningGuide:
    iterations: 15
    files:
      - FineTuningGuide.md
  README:
    iterations: 15
    files:
      - README.md
