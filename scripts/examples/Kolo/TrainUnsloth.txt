3. TrainUnsloth
Purpose:
This section details how to fine-tune a model using Unslothâ€”an open-source tool integrated into Kolo designed for efficient LLM fine-tuning with lower VRAM requirements.

Key Details:

Overview:
Unsloth is optimized for faster training and requires less GPU memory, making it ideal for local model fine-tuning.

Pre-requisites:

Ensure your training data has been copied into the container using the copy training data command (see below).
Basic Command:

./train_model_unsloth.ps1 -OutputDir "GodOutput" -Quantization "Q4_K_M" -TrainData "data.jsonl"
Detailed Command (with all available parameters):

./train_model_unsloth.ps1 -Epochs 3 -LearningRate 1e-4 -TrainData "data.jsonl" -BaseModel "unsloth/Llama-3.2-1B-Instruct-bnb-4bit" -ChatTemplate "llama-3.1" -LoraRank 16 -LoraAlpha 16 -LoraDropout 0 -MaxSeqLength 1024 -WarmupSteps 10 -SaveSteps 500 -SaveTotalLimit 5 -Seed 1337 -SchedulerType "linear" -BatchSize 2 -OutputDir "GodOutput" -Quantization "Q4_K_M" -WeightDecay 0
What Happens:
This script starts the fine-tuning process using Unsloth, applying the specified parameters such as epochs, learning rate, and quantization type. The output (trained model data) is stored in the designated directory (GodOutput).